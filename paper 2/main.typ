#import "template.typ": *

// Take a look at the file `template.typ` in the file panel
// to customize this template and discover how it works.
#show: project.with(
  title: "Understanding Transformers: The Math behind ChatGPT",
  authors: (
    (name: "Grant Yang", affiliation: "Period 4"),
  ),
)

#set math.equation(numbering: "(Eq. 1)")

// We generated the example code below so you can see how
// your document will look. Go ahead and replace it with
// your own content!



= Introduction
First proposed in the famous 2017 paper "Attention is All you Need" by Vaswani et al. @vaswani2023attention, the transformer architecture is the basis of all modern large language models. Originally intended as an architecture to tackle machine translation, it has proven to be incredibly generalizable and has found applications in diverse areas ranging from computer vision (in Vision Transformers) to audio transcription (in OpenAI's Whisper). In this paper, I aim to give the reader a solid understanding of the mathematics and low-level mechanisms that power these cutting-edge technologies.

= Overview
Fundamentally, transformers operate on a set of nodes (i.e. vectors), which are arranged in some graph. Computations are then performed in two steps: exchanging information using an _attention mechanism_, followed by individual processing. Thus, the connections in the graph would indicate how information can flow between nodes during the information exchange step. After the initial talking phase, there is a thinking phase, where each node is individually processed using a traditional feed-forward neural network. These two steps make up a single _block_ of computation, which is repeated multiple times.

The end result is a transformed version of the original set of nodes. In the case of a large language model, each node would represent a vectorized word or _token_, so an LLM might transform ["A", "quick", "brown", "fox"] into ["quick", "brown", "fox", "jumped"], repeating this process until an end-of-stream token is reached. However, a complication is the fact that transformers have no sense of the order of the nodes, so a _positional encoding_ must be added to the tokens before they are passed to the transformer. Training a transformer to predict the next token is called _pre-training_, and modern chatbots also undergo a _fine-tuning_ step where human feedback is incorporated using the _proximal policy optimization_ reinforcement learning algorithm. 

// This abstract way of thinking about transformers as operating on graphs explains why they are so generalizable and multimodal: not only can the nodes in the graph be words, but they can also be segments of images, audio clips, or anything else.

= The Attention Mechanism

As a first step for performing attention, nodes generate three vectors: the _query_, _key_, and _value_ vectors. The query vector encodes the type of information that the node is looking for, the key vector encodes the type of information the node has, and the value vector encodes the actual content of the node's information. Each node that is looking to share information generates a key and a value vector, and each node that is looking to gather information generates a query vector. Note that nodes can both gather information and share information at the same time. The query and key vectors (but not the value vector) must have the same dimension, since we will need to take a dot product between them to see if a node is sharing the information that another node is looking for.

To formalize this and avoid any confusion between the node's value and the node's value vector, I will write the $y$th node's value as $arrow(n_y)$ and its corresponding value vector as $arrow(v_y)$. I will use this index $y$ to index nodes that are sharing information and the index $x$ for nodes that are gathering information. Thus, the query and key vectors will be written as $arrow(q_x)$ and $arrow(k_y)$ respectively. All three of these vectors are generated by simple matrix multiplications, where the values of the nodes $arrow(n_x)$ or $arrow(n_y)$ are left-multiplied by a learnable weight matrix:

$ 
arrow(q_x) = W_Q arrow(n_x) 
$ 
$
arrow(k_y) = W_K arrow(n_y)
$
$
arrow(v_y) = W_V arrow(n_y)
$

To aggregate all the information that it gathers into a single vector $arrow(a_x)$, the node $x$ takes the weighted average of the value vectors $arrow(v_y)$ of all the nodes $y$ sharing information, weighted by a corresponding relevance score. This relevance score is computed by taking the dot product between the information-gathering node's query vector $arrow(q_x)$ and the information-sharing node's key vector $arrow(k_y)$. Summing $y$ across all nodes that are sharing information to node $x$, we want to compute something similar to this sum:

$
arrow(a_x) "is similar to" sum_(y) (arrow(q_x) dot arrow(k_y)) arrow(v_y)
$

To compute the true average $arrow(a_x)$ instead of this simplified sum, we use a function called _softmax_. The softmax function operates on vectors and maps all components into the range $[0, 1]$ while ensuring they sum to $1$. The exponential function $e^x$ is used to map negative values into the positive range. The $j$th component of a softmaxed vector can be expressed as the following, where $k$ sums across the entire vector $arrow(u)$, and $u_j$ and $u_k$ are the $j$th and $k$th components:

$
"softmax"(arrow(u))_j = exp(u_j)/ (sum_k exp(u_k))
$ <softmax>

For the weighted average, we define a vector of all the relevance scores $arrow(q_x) dot arrow(k_y)$ and apply softmax to it. One can think of taking this vector and left-multiplying by a matrix of $arrow(v_y)$ vectors to compute the average:

$
arrow(r) "is a vector where each component" r_i = arrow(q_x) dot arrow(k_y_i)
$
$
arrow(a_x) = [arrow(v_y_1), arrow(v_y_2),...] "softmax"(arrow(r))
$

More sensibly, the weighted average $arrow(a_x)$ for the node $x$ could be written as:
$
arrow(a_x) = (sum_(y) exp(arrow(q_x) dot arrow(k_y)) arrow(v_y))/ (sum_y exp(arrow(q_x) dot arrow(k_y)))
$

In practice, this $arrow(a_x)$ value is scaled by a factor of $1 slash.big sqrt(d_k)$, where $d_k$ is the number of dimensions in the key vector. This keeps intermediate values in sensible ranges and makes training easier. The entire process of calculating the $arrow(a_x)$ value and scaling is called _scaled dot product attention_, and it makes up a single _attention head._

Multiple attention heads can be used for _multiheaded attention._ Each head uses its own set of learnable $W_Q$, $W_K$, and $W_V$ matrices to produce query, key, and value vectors and calculate the attention (i.e. the weighted averages $arrow(a_x)$). In theory, each attention head would be responsible for exchanging certain key bits of information. The $arrow(a_x)$ vectors from all of the attention heads are concatenated together and left-multiplied by a learnable matrix $W_O$ in a process called _output projection_. Finally, this value is added onto the node's value $arrow(n_x)$:

$
arrow(n_x) -> arrow(n_x) + W_O "Concat"(arrow(a_x) " from head"_1, arrow(a_x) " from head"_2, ...)
$

This concludes the attention mechanism, or talking step, as information from all of the $arrow(n_y)$ vectors has been added into $arrow(n_x)$. 

== Individual Processing

Next, we move onto the thinking step: the node's value $arrow(n_x)$ is fed into a small two-layer neural network, and the output is added onto the node's value again. With some activation function $f$ (usually ReLU or some variant of it), we can write this step mathematically as:

$
f(x) = "ReLU"(x) = cases(
  x "if" x >= 0,
  0 "if" x < 0,
)
$
$
arrow(n_x) -> arrow(n_x) + W_2 f(W_1 arrow(n_x) + arrow(b_1)) + arrow(b_2)
$


The weight matrices $W_1$ and $W_2$ as well as the bias vectors $arrow(b_1)$ and $arrow(b_2)$ are learnable, but the bias vectors are sometimes omitted. The threshold function is applied element-wise on the vector, and we skip applying it the second time to avoid limiting the output range.

Only using addition to modify the nodes' values helps to address the _vanishing gradient_ problem, where the partial derivative of the cost function with respect to parameters shrinks to zero as neural networks grow deeper @gradient. Because the final output is effectively a sum of the outputs of all intermediate layers, the gradient does not shrink as much when the transformer grows deeper. Each parameter is more directly connected to the final output, requiring fewer chain rule multiplications.

== Layer Normalization

Another technique, _layer normalization_, is used to further alleviate the vanishing gradient problem @layernorm. For each application of this technique, two new learnable scalar parameters are introduced, $gamma$ and $beta$. The layer's output is scaled and biased so that it has a mean of $beta$ and a standard deviation of $|gamma|$. To apply layer normalization, the following formula is applied to each component $x$ of the layer:

$ y = beta + gamma * (x - "E"[x]) / sqrt("Var"[x] + 10^(-5)) $

$"E"[x]$ is the expected value (mean) and $"Var[x]"$ is the variance (the square of standard deviation). Values for the mean and variance are estimated using past outputs produced by the layer. A small factor $epsilon$ (in this case $10^(-5)$) is added to avoid division by zero.

In the original 2017 paper, layer normalization was applied to $arrow(n_x)$ node values twice per block: once after adding the $arrow(a_x)$ attention values and again after adding the feed-forward network @vaswani2023attention. However, many modern transformers use a different model proposed in a later paper, which showed that applying layer normalization before performing attention and before the feed-forward network provides better results and faster training times @onlayernorm. Only the values passed into the attention mechanism and neural network are normalized, not the stored values of $arrow(n_x)$.


== Matrix Form of Attention
Attention can be efficiently calculated as a series of matrix multiplications. We can pack the query, key, and value vectors for all nodes together as rows in the matrices $Q$, $K$, and $V$. In other words, for $x$ nodes gathering information and $y$ nodes sharing, and with $d_k$-dimensional keys and $d_v$-dimensional values, the matrix $Q$ is $x$ by $d_k$, $K$ is $y$ by $d_k$, and $V$ is $y$ by $d_v$. Then, an attention head can be written as:

$
"Attention"(Q,K,V)="softmax"((Q K^top)/sqrt(d_k)) V
$

where softmax is applied along the rows of the $x$ by $y$ matrix (i.e. across $y$). Refer back to @softmax for the mathematical definition of softmax. This matrix is a table of relevance weightings between information-sharing and information-gathering nodes.

Although this $"Attention"(Q,K,V)$ form is mathematically equivalent to computing the $arrow(a_x)$ values as discussed previously, attention is often written in this matrix form @vaswani2023attention.

== Encoder vs. Decoder Architectures

Large language models are typically _decoder-only_ transformers. In this case, the graph of connections has each node connected to all nodes that come before it but not any of the nodes after it. This pattern of attention is called _masked self-attention_ as the nodes that come later are masked to the nodes before them. This way, each node can only look backwards in time, and the decoder can be used to generate text. A transformer can be trained to predict the next token given the last token and the ability to perform attention on the preceding tokens.

By contrast, in an _encoder_ transformer, each node is connected to every other node. Encoders are used to perform tasks like sentiment analysis where no information needs to be hidden. The patterns of attention in both encoders and decoders are types of _causal self-attention_, as the sentence is performing attention within itself, and in theory words should attend to what logically modify them.

The original 2017 paper contained both an encoder and a decoder, so it had an additional component of _cross-attention_, which occurred in the decoder after masked self-attention and before the feed-forward network. The encoder processes the foreign-language text, and the decoder outputs the translated text. In cross-attention, the nodes outputted by the encoder network share information using key and value vectors, while the nodes in the decoder gather information with query vectors. Information is transferred across from the encoder to the decoder.

= Conclusion

What I have presented in this paper has been an incomplete picture of modern large language models. There are still important training methods that I have not covered, like _dropout_ (randomly zeroing values to avoid overfitting) and the _ADAM optimizer_ (an improved algorithm for updating weights more sophisticated than simply adding partial derivatives). As mentioned in the overview, neither have I covered aspects like fine-tuning, tokenization, or positional encoding. Nevertheless, I hope that this paper has captured the core ideas of the transformer architecture and scaled dot product attention. For more details and code, OpenAI's Andrej Karpathy has a video lecture where he builds a simple transformer in PyTorch, which was an extremely helpful resource to me when writing this paper @karpathygpt. #pagebreak()

#bibliography("bib.bib")
// @kingma2017adam
// @vaswani2023attention
// @dosovitskiy2021image