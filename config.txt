do_training: true
max_iterations: int 1024
error_cutoff: float 8e-5
learn_rate: float 3e-3
batch_size: int 30
dropout: float 0.06
betas: float[0.99, 0.995]
weight_decay: float 0
epsilon: float 1e-5

print_weights: false

# how often to print out training loss (in # of iterations)
printout_period: int 1
checkpoint_period: int 2
label0: float 0.02
label1: float 0.98

network_topology: int[16384,24,24,5]

# "sigmoid", "tanh", or "leaky_relu"
activation_function: "sigmoid"

# "randomize", "from_file", "smart_random"
initialization_mode: "smart_random"
rand_lo: float -0.001
rand_hi: float 0.001

# load_file is not used with initialization_mode="randomize"
load_file: "softmax-1L.net"
save_file: "softmax-1L.net"

# In the future, I will implement loading training data from a binary file or directory.
dataset_mode: "from_file"
dataset_file: "dataset/.secret.data"
case [0,0]: float[0,0,0]
case [0,1]: float[0,1,1]
case [1,0]: float[0,1,1]
case [1,1]: float[1,1,0]


# expected results for configs
# 2-5-3    ~40-60k its
# 2-20-3   ~20k its
# 2-100-3  1,1,1,---   0.75 err
# 2-2-3    0.719
# 2-1-3    0.3,0.8,0.5
# A>2
# 2-10-3   train/save then load weights
